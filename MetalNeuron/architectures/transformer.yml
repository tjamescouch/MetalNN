name: "Composable Transformer"

layers:
  # Initial embedding of tokens into vector space
  - type: Embedding
    input_size: "vocab_size"
    output_size: 512

  # Positional encoding explicitly provides sequence-order information
  - type: PositionalEncoding
    embedding_dim: 512
    max_sequence_length: 1024

  # Multi-head attention layer
  - type: MultiHeadAttention
    input_size: 512
    output_size: 512
    num_heads: 8
    dropout_rate: 0.1

  # Add & Norm (Residual Connection explicitly shown)
  - type: ResidualConnection
    layers:
      - type: Dropout
        rate: 0.1
        input_size: 512
        output_size: 512

  - type: LayerNorm
    input_size: 512
    output_size: 512

  # Explicitly broken-down Feed-Forward Network
  - type: Dense
    input_size: 512
    output_size: 2048
    activation: gelu
    initializer: he

  - type: Dropout
    rate: 0.1
    input_size: 2048
    output_size: 2048

  - type: Dense
    input_size: 2048
    output_size: 512
    activation: linear
    initializer: xavier

  - type: LayerNorm
    input_size: 512
    output_size: 512

  # Additional Transformer Block explicitly shown (Stacking clearly indicated)
  - type: MultiHeadAttention
    input_size: 512
    output_size: 512
    num_heads: 8
    dropout_rate: 0.1

  - type: ResidualConnection
    layers:
      - LayerNorm
      - Dropout
    dropout_rate: 0.1

  # Another explicitly broken-down Feed-Forward layer
  - type: Dense
    input_size: 512
    output_size: 2048
    activation: gelu
    initializer: he

  - type: Dropout
    rate: 0.1
    input_size: 2048
    output_size: 2048

  - type: Dense
    input_size: 2048
    output_size: 512
    activation: linear
    initializer: xavier

  - type: LayerNorm
    input_size: 512
    output_size: 512

  # Final Dense projection for classification
  - type: Dense
    input_size: 512
    output_size: 10
    activation: softmax
    initializer: xavier

training:
  optimizer:
    type: adamw
    learning_rate: 0.0001
    parameters:
      beta1: 0.9
      beta2: 0.999
      weight_decay: 0.01

  scheduler:
    type: cosine_annealing
    warmup_steps: 4000

  epochs: 10
  batch_size: 64

dataset:
  type: "text"
  training_data: "train.txt"
  validation_data: "valid.txt"
  tokenizer: "tokenizer.json"

metadata:
  author: "James Couch"
  description: "Explicit Transformer architecture with clearly broken-down Feed-Forward layers and explicit residual connections."
